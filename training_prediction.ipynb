{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D, LSTM, Dense, Dropout, LayerNormalization, Input\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.regularizers import l2, l1_l2,l1\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "#Connects to the PostgreSQL database and retrieves the historical price data of a specific cryptocurrency.\n",
    "#Returns a DataFrame with the data sorted by date.\n",
    "def fetch_data(ticker, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM crypto_prices\n",
    "    WHERE ticker = '{ticker}'\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "#Converts the date column into a time index, selects relevant features,\n",
    "#and scales them using MaxAbsScaler to normalize the values.\n",
    "def preprocess_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    features = df[['features']] \n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    return scaled_features\n",
    "    \n",
    "#Generates training sets using a defined time window, time_step, to predict\n",
    "#the closing price (column 3) based on previous data.\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), :])\n",
    "        y.append(data[i + time_step, 3]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#Calculates the sMAPE (Symmetric Mean Absolute Percentage Error) metric\n",
    "#to evaluate the mean percentage error between predicted and actual values.\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = tf.abs(y_true - y_pred)\n",
    "    denominator = (tf.abs(y_true) + tf.abs(y_pred)) / 2\n",
    "    smape_value = tf.where(tf.equal(denominator, 0), tf.zeros_like(numerator), numerator / denominator)\n",
    "    return tf.reduce_mean(smape_value) * 100  # Convert to percentage\n",
    "\n",
    "#Defines the MAE (Mean Absolute Error) loss metric.\n",
    "def mean_absolute_error_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "#Custom callback that reduces the L2 regularization value in specific layers at the end of each epoch,\n",
    "#adjusting the L2 penalty to avoid overfitting\n",
    "class L2DecayCallback(Callback):\n",
    "    def __init__(self, initial_l2, decay_rate, target_layers):\n",
    "        super().__init__()\n",
    "        self.initial_l2 = initial_l2\n",
    "        self.decay_rate = decay_rate\n",
    "        self.target_layers = target_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_l2 = self.initial_l2 * (self.decay_rate ** epoch)\n",
    "        for layer in self.target_layers:\n",
    "            if hasattr(layer, 'kernel_regularizer'):\n",
    "                layer.kernel_regularizer.l2 = current_l2\n",
    "\n",
    "#Builds a sequential model with Conv1D, LSTM layers, and a multi-head attention layer,\n",
    "#with L2 regularization that decays during training.\n",
    "#Uses Spatial Dropout to reduce overfitting and L1 regularization in the multi-head attention layer\n",
    "#to improve feature selection.\n",
    "#Utilizes Keras Tuner to optimize hyperparameters like the number of filters and learning rate.\n",
    "def build_model_with_decay(hp, X_train, initial_l2, decay_rate):\n",
    "    # Definición de la entrada\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    # Conv1D layer with Spatial Dropout\n",
    "    conv_layer = Conv1D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(initial_l2)\n",
    "    )(inputs)\n",
    "    conv_layer = SpatialDropout1D(rate=0.2)(conv_layer)  # Add Spatial Dropout\n",
    "\n",
    "    # LSTM layer with Spatial Dropout\n",
    "    lstm_layer = LSTM(\n",
    "        units=hp.Int('units_layer_1', min_value=50, max_value=500, step=50),\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=l2(initial_l2)\n",
    "    )(conv_layer)\n",
    "    lstm_layer = SpatialDropout1D(rate=0.2)(lstm_layer)  # Add Spatial Dropout\n",
    "\n",
    "    # multi-head attention layer\n",
    "    attention_layer = MultiHeadAttention(\n",
    "        num_heads=hp.Int('num_heads', min_value=2, max_value=8, step=1),\n",
    "        key_dim=hp.Int('key_dim', min_value=8, max_value=64, step=8),\n",
    "        kernel_regularizer=l1(0.01)\n",
    "    )(lstm_layer, lstm_layer)  # Applies attention to the output of the LSTM layer.\n",
    "\n",
    "    # Normalization after the attention layer.\n",
    "    attention_output = LayerNormalization()(attention_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, kernel_regularizer=l2(initial_l2))(attention_output)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(\n",
    "        learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "    ), loss='mean_squared_error')\n",
    "\n",
    "    return model \n",
    "\n",
    "\n",
    "#Stores error metrics in a database table, allowing for the analysis and \n",
    "# comparison of the performance of different models.\n",
    "def store_errors(ticker, mse, mae, r2, mape, smape_val, conn_params):\n",
    "    mse = float(mse)\n",
    "    mae = float(mae)\n",
    "    r2 = float(r2)\n",
    "    mape = float(mape)\n",
    "    smape_val = float(smape_val)\n",
    "    \n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    INSERT INTO error (ticker, mse, mae, r2, mape, smape, coment)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, ' ');\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (ticker, mse, mae, r2, mape, smape_val))\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "#Performs the full cycle of training and evaluation for each cryptocurrency in the list of tickers.\n",
    "#Splits the data into training and validation sets, uses Keras Tuner for hyperparameter optimization via Bayesian search,\n",
    "#evaluates the model with performance metrics, and saves the best model to a file.\n",
    "def train_and_evaluate(tickers, conn_params, time_step=48):\n",
    "    for ticker in tickers:\n",
    "        print(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "        df = fetch_data(ticker, conn_params)\n",
    "        scaled_features = preprocess_data(df)\n",
    "        X, y = create_dataset(scaled_features, time_step)\n",
    "        \n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "        #Defines the tuner using Bayesian Optimization.\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            lambda hp: build_model_with_decay(hp,\n",
    "            X_train,\n",
    "            0.01,\n",
    "            decay_rate=0.9 \n",
    "            ),\n",
    "            objective='val_loss',\n",
    "            max_trials=50,  \n",
    "            directory='my_dir',\n",
    "            project_name=f'crypto_lstm_{ticker}'\n",
    "        )\n",
    "\n",
    "        # Performs hyperparameter search.\n",
    "        fixed_batch_size = 128\n",
    "        tuner.search(X_train, y_train, epochs=150, validation_split=0.2, \n",
    "                     batch_size=fixed_batch_size,\n",
    "                     callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "        # Evaluate the best model\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        \n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mape = np.mean(np.abs((y_val - y_pred) / y_val)) * 100\n",
    "\n",
    "        smape_val = smape(tf.convert_to_tensor(y_val, dtype=tf.float32), \n",
    "                          tf.convert_to_tensor(y_pred, dtype=tf.float32)).numpy()\n",
    "        \n",
    "        print(f\"Ticker: {ticker}, MSE: {mse}, MAE: {mae}, R²: {r2}, MAPE: {mape}, sMAPE: {smape_val}\")\n",
    "\n",
    "        #Save the best model\n",
    "        model_path = f\"model_{ticker}.keras\"\n",
    "        best_model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        store_errors(ticker, mse, mae, r2, mape, smape_val, conn_params)\n",
    "\n",
    "# DB Params\n",
    "conn_params = {\n",
    "    'dbname': 'dbname',\n",
    "    'user': 'user',\n",
    "    'password': 'password',\n",
    "    'host': 'localhost',\n",
    "    'port': 'port'\n",
    "}\n",
    "tickers = [\n",
    "\"BTCUSDT\" #Bitcoin\n",
    "]\n",
    "train_and_evaluate(tickers, conn_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "#Connects to the PostgreSQL database and retrieves historical \n",
    "# price data of a specific cryptocurrency, ordered by date.\n",
    "def fetch_data(ticker, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM crypto_prices\n",
    "    WHERE ticker = '{ticker}'\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "#Retrieves the timestep value from the \"time1\" table in the database for a given ticker.\n",
    "def fetch_timestep(ticker, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    query = f\"\"\"\n",
    "    SELECT timestep FROM timestep\n",
    "    WHERE ticker = '{ticker}';\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    timestep = cursor.fetchone()[0]  \n",
    "    conn.close()\n",
    "    return int(timestep)  # Convert timestep to int\n",
    "\n",
    "\n",
    "#Preprocesses the data by converting dates, scaling features, and\n",
    "#returning the last actual price along with the scaler used.\n",
    "def preprocess_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    features = df[['features']]\n",
    "    \n",
    "    # Apply scaling.\n",
    "    scaler = MaxAbsScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    return scaled_features, features['close'].values[-1], scaler  \n",
    "\n",
    "#Loads a previously saved model in Keras format, customized with the sMAPE and MAE metrics.\n",
    "def load_model(ticker):\n",
    "    model_path = f\"modelo_{ticker}.keras\"\n",
    "    return tf.keras.models.load_model(model_path, custom_objects={\"smape\": smape, \"mean_absolute_error_tf\": mean_absolute_error_tf})\n",
    "\n",
    "# Defines the method to correctly load a saved model with custom metrics.\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = tf.abs(y_true - y_pred)\n",
    "    denominator = (tf.abs(y_true) + tf.abs(y_pred)) / 2\n",
    "    return tf.reduce_mean(numerator / denominator) * 100  \n",
    "\n",
    "# Defines the method to correctly load a saved model with custom metrics.\n",
    "def mean_absolute_error_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "#Calculates the percentage of profit or loss based on predicted and actual prices, ensuring no division by zero occurs.\n",
    "def calculate_profit_percentage(predicted_price, actual_price):\n",
    "    if actual_price != 0:\n",
    "        return ((predicted_price - actual_price) / actual_price) * 100\n",
    "    return 0  # Prevents division by zero.\n",
    "\n",
    "# Inserts the profit or loss percentage into the \"profit\" table of the database.\n",
    "def store_profit(ticker, profit, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    INSERT INTO profit (ticker, profit)\n",
    "    VALUES (%s, %s);\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (ticker, profit))\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "#Performs the prediction process for a list of tickers, preprocesses data, loads the model, predicts the next price,\n",
    "#calculates the profit or loss, and stores the results in the database.\n",
    "def prediction_module(tickers, conn_params):\n",
    "    for ticker in tickers:\n",
    "        print(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "    \n",
    "        df = fetch_data(ticker, conn_params)\n",
    "        scaled_features, actual_price, scaler = preprocess_data(df)\n",
    "\n",
    "        time_step = fetch_timestep(ticker, conn_params)\n",
    "\n",
    "        \n",
    "        #Makes predictions for the next interval using the timestep obtained.\n",
    "        if len(scaled_features) >= time_step:  \n",
    "            last_data = scaled_features[-time_step:].reshape(1, time_step, scaled_features.shape[1])  \n",
    "        else:\n",
    "            print(f\"Not enough data to make a prediction for ticker: {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # Load saved model\n",
    "        model = load_model(ticker)\n",
    "\n",
    "        # Make prediction\n",
    "        predicted_price_scaled = model.predict(last_data)[0, 0]\n",
    "        predicted_price = scaler.inverse_transform(np.array([[predicted_price_scaled] + [0] * (scaled_features.shape[1] - 1)])[0].reshape(1, -1))[0][0]\n",
    "\n",
    "        # Calculates the profit or loss based on the predicted price and the actual price.\n",
    "        profit_percentage = calculate_profit_percentage(predicted_price, actual_price)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Ticker: {ticker}, Predicted Price: {predicted_price}, Actual Price: {actual_price}, Profit/Loss: {profit_percentage}%\")\n",
    "\n",
    "        # Store profit/loss on the DB\n",
    "        store_profit(ticker, profit_percentage, conn_params)\n",
    "\n",
    "# DB Params\n",
    "conn_params = {\n",
    "    'dbname': 'crypto8',\n",
    "    'user': 'postgres',\n",
    "    'password': 'loquesea',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "tickers = [\n",
    "\"BTCSDT\" #Bitcoin\n",
    "]  # Tickers list\n",
    "\n",
    "prediction_module(tickers, conn_params) #Run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
