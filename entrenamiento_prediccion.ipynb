{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D, LSTM, Dense, Dropout, LayerNormalization, Input\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.regularizers import l2, l1_l2,l1\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Conecta a la base de datos PostgreSQL y recupera los datos históricos de precios de una criptomoneda específica. \n",
    "# Devuelve un DataFrame con los datos ordenados por fecha.\n",
    "def fetch_data(ticker, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM crypto_prices\n",
    "    WHERE ticker = '{ticker}'\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "#  Convierte la columna de fecha en un índice de tiempo, selecciona características relevantes\n",
    "#  y las escala usando MaxAbsScaler para normalizar los valores.\n",
    "def preprocess_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    features = df[['caracteristicas']]\n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    return scaled_features\n",
    "    \n",
    "# Genera conjuntos de entrenamiento con una ventana temporal definida, time_step, para predecir \n",
    "# el precio de cierre (columna 3) basado en los datos anteriores.\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), :])\n",
    "        y.append(data[i + time_step, 3])  # Asumiendo que la columna 3 es el precio de cierre\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Calcula la métrica sMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "#  para evaluar el error porcentual medio entre valores predichos y reales.\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = tf.abs(y_true - y_pred)\n",
    "    denominator = (tf.abs(y_true) + tf.abs(y_pred)) / 2\n",
    "    smape_value = tf.where(tf.equal(denominator, 0), tf.zeros_like(numerator), numerator / denominator)\n",
    "    return tf.reduce_mean(smape_value) * 100  # Convertir a porcentaje\n",
    "\n",
    "# Define métrica de perdida MAE\n",
    "def mean_absolute_error_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "# Callback personalizado que reduce el valor de la regularización L2 en las capas específicas \n",
    "# al final de cada época, ajustando la penalización L2 para evitar el sobreajuste\n",
    "class L2DecayCallback(Callback):\n",
    "    def __init__(self, initial_l2, decay_rate, target_layers):\n",
    "        super().__init__()\n",
    "        self.initial_l2 = initial_l2\n",
    "        self.decay_rate = decay_rate\n",
    "        self.target_layers = target_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_l2 = self.initial_l2 * (self.decay_rate ** epoch)\n",
    "        for layer in self.target_layers:\n",
    "            if hasattr(layer, 'kernel_regularizer'):\n",
    "                layer.kernel_regularizer.l2 = current_l2\n",
    "\n",
    "# Construye un modelo secuencial con capas Conv1D, LSTM y una capa de atención multi-cabezal, \n",
    "# con regularización L2 que decae durante el entrenamiento.\n",
    "# Se utiliza Spatial Dropout para reducir la posibilidad de sobre ajuste y l1 en capa de atención multi-cabezal\n",
    "# para mejorar la selección de características\n",
    "# Utiliza Keras Tuner para optimizar hiperparámetros como el número de filtros y la tasa de aprendizaje.\n",
    "def build_model_with_decay(hp, X_train, initial_l2, decay_rate):\n",
    "    # Definición de la entrada\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    # Capa Conv1D con Spatial Dropout\n",
    "    conv_layer = Conv1D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(initial_l2)\n",
    "    )(inputs)\n",
    "    conv_layer = SpatialDropout1D(rate=0.2)(conv_layer)  # Añadir Spatial Dropout\n",
    "\n",
    "    # Capa LSTM con Spatial Dropout\n",
    "    lstm_layer = LSTM(\n",
    "        units=hp.Int('units_layer_1', min_value=50, max_value=500, step=50),\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=l2(initial_l2)\n",
    "    )(conv_layer)\n",
    "    lstm_layer = SpatialDropout1D(rate=0.2)(lstm_layer)  # Añadir Spatial Dropout\n",
    "\n",
    "    # Capa de Atención Multihead\n",
    "    attention_layer = MultiHeadAttention(\n",
    "        num_heads=hp.Int('num_heads', min_value=2, max_value=8, step=1),\n",
    "        key_dim=hp.Int('key_dim', min_value=8, max_value=64, step=8),\n",
    "        kernel_regularizer=l1(0.01)\n",
    "    )(lstm_layer, lstm_layer)  # Aplicar atención sobre la salida de LSTM\n",
    "\n",
    "    # Normalización después de la capa de atención\n",
    "    attention_output = LayerNormalization()(attention_layer)\n",
    "\n",
    "    # Capa de salida\n",
    "    output = Dense(1, kernel_regularizer=l2(initial_l2))(attention_output)\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Compilación del modelo\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(\n",
    "        learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "    ), loss='mean_squared_error')\n",
    "\n",
    "    return model  # Solo retorna el modelo\n",
    "\n",
    "# Almacena las métricas de error en una tabla de la base de datos, lo que permite analizar \n",
    "# y comparar el rendimiento de diferentes modelos.\n",
    "def store_errors(ticker, mse, mae, r2, mape, smape_val, conn_params):\n",
    "    mse = float(mse)\n",
    "    mae = float(mae)\n",
    "    r2 = float(r2)\n",
    "    mape = float(mape)\n",
    "    smape_val = float(smape_val)\n",
    "    \n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    INSERT INTO error (ticker, mse, mae, r2, mape, smape, coment)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, ' ');\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (ticker, mse, mae, r2, mape, smape_val))\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Realiza el ciclo completo de entrenamiento y evaluación para cada criptomoneda en la lista de tickers.\n",
    "#  Divide los datos en conjuntos de entrenamiento y validación, usa Keras Tuner para buscar la mejor \n",
    "# configuración de hiperparámetros utilizando busqueda bayesiana, evalúa el modelo con métricas de rendimiento y guarda el mejor \n",
    "# modelo en un archivo.\n",
    "def train_and_evaluate(tickers, conn_params, time_step=48):\n",
    "    for ticker in tickers:\n",
    "        print(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "        df = fetch_data(ticker, conn_params)\n",
    "        scaled_features = preprocess_data(df)\n",
    "        X, y = create_dataset(scaled_features, time_step)\n",
    "        \n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "        # Definir el tuner usando Bayesian Optimization\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            lambda hp: build_model_with_decay(hp,\n",
    "            X_train,\n",
    "            0.01,\n",
    "            decay_rate=0.9 \n",
    "            ),\n",
    "            objective='val_loss',\n",
    "            max_trials=50,  # Número máximo de combinaciones a probar\n",
    "            directory='my_dir',\n",
    "            project_name=f'crypto_lstm_{ticker}'\n",
    "        )\n",
    "\n",
    "        # Realizar la búsqueda de hiperparámetros\n",
    "        fixed_batch_size = 128\n",
    "        tuner.search(X_train, y_train, epochs=150, validation_split=0.2, \n",
    "                     batch_size=fixed_batch_size,\n",
    "                     callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "        # Evaluar el mejor modelo\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        \n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mape = np.mean(np.abs((y_val - y_pred) / y_val)) * 100\n",
    "\n",
    "        smape_val = smape(tf.convert_to_tensor(y_val, dtype=tf.float32), \n",
    "                          tf.convert_to_tensor(y_pred, dtype=tf.float32)).numpy()\n",
    "        \n",
    "        print(f\"Ticker: {ticker}, MSE: {mse}, MAE: {mae}, R²: {r2}, MAPE: {mape}, sMAPE: {smape_val}\")\n",
    "        \n",
    "        model_path = f\"model_{ticker}.keras\"\n",
    "        best_model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        store_errors(ticker, mse, mae, r2, mape, smape_val, conn_params)\n",
    "\n",
    "# Parametros de conexion a BDD\n",
    "conn_params = {\n",
    "    'dbname': 'dbname',\n",
    "    'user': 'user',\n",
    "    'password': 'password',\n",
    "    'host': 'localhost',\n",
    "    'port': 'port'\n",
    "}\n",
    "tickers = [\n",
    "\"PAXGUSDT\"\n",
    "]\n",
    "train_and_evaluate(tickers, conn_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "#Conecta a la base de datos PostgreSQL y recupera datos históricos de \n",
    "# precios de una criptomoneda específica, ordenados por fecha.\n",
    "def fetch_data(ticker, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM crypto_prices\n",
    "    WHERE ticker = '{ticker}'\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "#Recupera el valor de timestep desde la tabla timestep en la base de datos para un ticker dado.\n",
    "def fetch_timestep(ticker, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    query = f\"\"\"\n",
    "    SELECT timestep FROM timestep\n",
    "    WHERE ticker = '{ticker}';\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    timestep = cursor.fetchone()[0]  # Obtener el primer resultado (timestep)\n",
    "    conn.close()\n",
    "    return int(timestep)  # Convertir el timestep a entero\n",
    "\n",
    "\n",
    "# Preprocesa los datos, convirtiendo fechas, escalando características y \n",
    "# devolviendo el último precio real junto con el escalador utilizado.\n",
    "def preprocess_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    features = df[['caracteristicas']]\n",
    "    \n",
    "    # Aplicar el escalado\n",
    "    scaler = MaxAbsScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    return scaled_features, features['close'].values[-1], scaler  # Retornar el último precio real y el scaler\n",
    "\n",
    "# Carga un modelo previamente guardado en formato Keras, personalizado con las métricas sMAPE y MAE.\n",
    "def load_model(ticker):\n",
    "    model_path = f\"modelo_{ticker}.keras\"\n",
    "    return tf.keras.models.load_model(model_path, custom_objects={\"smape\": smape, \"mean_absolute_error_tf\": mean_absolute_error_tf})\n",
    "\n",
    "# Se define para cargar correctamente el modelo guardado con métricas personalizadas\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = tf.abs(y_true - y_pred)\n",
    "    denominator = (tf.abs(y_true) + tf.abs(y_pred)) / 2\n",
    "    return tf.reduce_mean(numerator / denominator) * 100  # Convertir a porcentaje\n",
    "\n",
    "# Se define para cargar correctamente el modelo guardado con métricas personalizadas\n",
    "def mean_absolute_error_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "# Calcula el porcentaje de ganancia o pérdida basado en los precios predichos y reales, evitando divisiones por cero.\n",
    "def calculate_profit_percentage(predicted_price, actual_price):\n",
    "    if actual_price != 0:\n",
    "        return ((predicted_price - actual_price) / actual_price) * 100\n",
    "    return 0  # Para evitar la división por cero\n",
    "\n",
    "# Inserta el porcentaje de ganancia o pérdida en la tabla profit de la base de datos.\n",
    "def store_profit(ticker, profit, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    INSERT INTO profit (ticker, profit)\n",
    "    VALUES (%s, %s);\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (ticker, profit))\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Realiza el proceso de predicción para una lista de tickers, preprocesa datos, carga el modelo, predice \n",
    "# el próximo precio y calcula la ganancia o pérdida, almacenando los resultados en la base de datos.\n",
    "def prediction_module(tickers, conn_params):\n",
    "    for ticker in tickers:\n",
    "        print(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "        # Obtener y preprocesar los datos\n",
    "        df = fetch_data(ticker, conn_params)\n",
    "        scaled_features, actual_price, scaler = preprocess_data(df)\n",
    "\n",
    "        # Obtener el timestep desde la tabla 'time'\n",
    "        time_step = fetch_timestep(ticker, conn_params)\n",
    "\n",
    "        # Hacer predicciones para el próximo intervalo con el timestep obtenido\n",
    "        if len(scaled_features) >= time_step:  # Asegurarse de tener suficientes datos para predecir\n",
    "            last_data = scaled_features[-time_step:].reshape(1, time_step, scaled_features.shape[1])  # Usar los últimos timestep\n",
    "        else:\n",
    "            print(f\"Not enough data to make a prediction for ticker: {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # Cargar el modelo guardado\n",
    "        model = load_model(ticker)\n",
    "\n",
    "        # Hacer la predicción\n",
    "        predicted_price_scaled = model.predict(last_data)[0, 0]\n",
    "\n",
    "        # Invertir el escalado del precio predicho\n",
    "        predicted_price = scaler.inverse_transform(np.array([[predicted_price_scaled] + [0] * (scaled_features.shape[1] - 1)])[0].reshape(1, -1))[0][0]\n",
    "\n",
    "        # Calcular la ganancia o pérdida\n",
    "        profit_percentage = calculate_profit_percentage(predicted_price, actual_price)\n",
    "\n",
    "        # Imprimir el resultado\n",
    "        print(f\"Ticker: {ticker}, Predicted Price: {predicted_price}, Actual Price: {actual_price}, Profit/Loss: {profit_percentage}%\")\n",
    "\n",
    "        # Almacenar la ganancia en la base de datos\n",
    "        store_profit(ticker, profit_percentage, conn_params)\n",
    "\n",
    "# Parámetros de conexión a BDD\n",
    "conn_params = {\n",
    "    'dbname': 'crypto8',\n",
    "    'user': 'postgres',\n",
    "    'password': 'loquesea',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "tickers = [\n",
    "\"BTCSDT\" #Bitcoin\n",
    "]  # Lista de tickers\n",
    "\n",
    "prediction_module(tickers, conn_params) #Ejecutar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
